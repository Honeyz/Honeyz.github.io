<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[idea下使用脚本在远程hadoop环境去执行和交互]]></title>
      <url>%2F2016%2F12%2F05%2Fidea%E4%B8%8B%E4%BD%BF%E7%94%A8%E8%84%9A%E6%9C%AC%E5%9C%A8%E8%BF%9C%E7%A8%8Bhadoop%E7%8E%AF%E5%A2%83%E5%8E%BB%E6%89%A7%E8%A1%8C%E5%92%8C%E4%BA%A4%E4%BA%92%2F</url>
      <content type="text"><![CDATA[title: idea下使用脚本在远程hadoop环境去执行和交互date: 2016-12-05 22:09:19tags:idea,hadoop,运行脚本 categories:hadoop在ideal下创建maven工程，编写mapreduce程序，调用虚拟机上hadoop分布式执行，避免每次都要进行打包，将文件、jar包上传到主节点master上，在主节点上再去调用 1.idea创建maven工程，目录结构如下 input 文件下放程序需要读取的文件，每个程序对应input下一个文件夹，如wordcount，words文件为com.zju.czx.WordCount 要读取的文件;output 下为程序执行完成从hdfs上放下的文件target 存放整个项目的打包生成的jarsrc 存放mapreduce的程序shell 存放执行脚本，通过脚本来对虚拟机上进行操作 pom.xml 使用的是hadoop版本1.2.1.123456789101112131415161718192021222324252627282930313233&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.zju.czx&lt;/groupId&gt; &lt;artifactId&gt;remoteHadoop&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;remoteHadoop&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;hadoop.version&gt;1.2.1&lt;/hadoop.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-core&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 2.在idea这个开发环境的机器上，与master设置ssh的免密码登录12345678# 在ssh下生成 id_rsa.pub、id_rsa ssh-keygen -t rsa # 将公钥复制到master下面 cp id_rsa.pub honeycc_ id_rsa.pub scp .ssh/honeycc_ id_rsa.pub czx-hadoop@192.168.149.135:~/.ssh/ # 在master上，将公钥添加到 authorized_keys上 cat .ssh/honeycc_ id_rsa.pub &gt;&gt; authorized_keys #设置成功 3.编写脚本，将开发环境与master进行开发run.sh1234567891011121314151617181920212223242526272829303132333435363738#!/bin/shjarname=$1main=$2dirctory=$3if [ "$#" -eq "3" ];then echo "你的jar包的名字是:$&#123;jarname&#125;" echo "你所要执行的主类:$&#123;main&#125;" echo "你所导入的文件目录:$&#123;dirctory&#125;" echo " 将 $&#123;jarname&#125; 上传到主节点的hadoop路径下 master ~/hadoop/" scp ../target/$&#123;jarname&#125; czx-hadoop@192.168.149.135:~/hadoop/ echo " 本地文件复制到主节点上 ../input/$&#123;dirctory&#125; czx-hadoop@192.168.149.135:~/input/$&#123;dirctory&#125;" scp -r ../input/$&#123;dirctory&#125; czx-hadoop@192.168.149.135:~/input/ echo "在主节点链接 hdfs 创建文件夹 ./hadoop/bin/hadoop fs -mkdir /input/$&#123;dirctory&#125;" ssh czx-hadoop@192.168.149.135 "./hadoop/bin/hadoop fs -mkdir /input/$&#123;dirctory&#125;" echo "将主节点上文件上传到hdfs上 ./hadoop/bin/hadoop fs -put ~/input/$&#123;dirctory&#125;/* /input/$&#123;dirctory&#125;/" ssh czx-hadoop@192.168.149.135 "./hadoop/bin/hadoop fs -put ~/input/$&#123;dirctory&#125;/* /input/$&#123;dirctory&#125;/" echo "删除输出目录 ./hadoop/bin/hadoop rm -r /output/$&#123;dirctory&#125;" ssh czx-hadoop@192.168.149.135 "./hadoop/bin/hadoop fs -rmr /output/$&#123;dirctory&#125;" echo " 执行$&#123;jarname&#125; ./hadoop/bin/hadoop jar hadoop/$&#123;jarname&#125; $&#123;main&#125; /input/$&#123;dirctory&#125;/* /output/$&#123;dirctory&#125;" ssh czx-hadoop@192.168.149.135 "./hadoop/bin/hadoop jar hadoop/$&#123;jarname&#125; $&#123;main&#125; /input/$&#123;dirctory&#125;/* /output/$&#123;dirctory&#125;" ssh czx-hadoop@192.168.149.135 "rm -r ~/output/$&#123;dirctory&#125;" echo "从hdfs取出执行完成的结果 ./hadoop/bin/hadoop fs -get /output/$&#123;dirctory&#125; ~/output/$&#123;dirctory&#125;" ssh czx-hadoop@192.168.149.135 "./hadoop/bin/hadoop fs -get /output/$&#123;dirctory&#125; ~/output/$&#123;dirctory&#125;" echo "将结果复制到本地" rm -r ../output/$&#123;dirctory&#125; scp -r czx-hadoop@192.168.149.135:~/output/$&#123;dirctory&#125;/ ../output/else echo "需要提供3个参数，你只提供了 $# 参数,第一个参数为jar名字，第二个为执行主函数 第三个为输入文件的文件夹名字 eg: remoteHadoop-1.0-SNAPSHOT.jar com.zju.czx.WordCount words"fi 总结这样一来就减少了每次再手动与虚拟机进行脚本，一个脚本就进行操作了。在执行上可能要按照规定好规则去执行。目录结构的创建也要按照规则去创建。不过自己使用起来还是很方便的，而且用maven对其进行打包控制，可以重复编写多个mapreduce程序，只要在执行脚本的时候设定好主类就好了。12./run.sh remoteHadoop-1.0-SNAPSHOT.jar com.zju.czx.WordCount words# jar包，主类类名，程序要读取的文件夹]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[多台虚拟机搭建hadoop环境]]></title>
      <url>%2F2016%2F11%2F28%2Fhadoop%2F</url>
      <content type="text"><![CDATA[Hadoop配置流程 修改hosts 建立hadoop运行帐号 ssh - 免密码 配置jdk hadoop运行环境 设置hadoop配置文件 通过ssh将配置同步到各节点 格式化namenode 启动hadoop，jps和网站检查运行情况 使用的是3台ubuntu系统进行hadoop环境的搭载、hadoop版本1.2.1 1.修改hosts可先进行主机hostname的修改，三台都进行设置。 /etc/hosts1234567891011127.0.0.1 localhost localhost.localdomain localhost#127.0.1.1 ubuntu192.168.149.135 master192.168.149.134 slave2 192.168.149.133 slave1 slave1.localdomain slave1::1 ip6-localhost ip6-loopbackfe00::0 ip6-localnetff00::0 ip6-mcastprefixff02::1 ip6-allnodesff02::2 ip6-allrouters 2.建立hadoop运行帐号为 hadoop 集群专门设置一个用户组及用户，用root设置添加用户的权限123456sudo groupadd hadoopsudo useradd –s /bin/bash –d /home/hadoop –m czx-hadoop –g hadoopsudo passwd czx-hadoopvim /etc/sudoersczx-hadoop ALL=(ALL) ALL #修改用户权限 3 个虚机结点均需要进行以上步骤来完成 hadoop 运行帐号的建立。 3.ssh免密码登录需要下载sudo apt-get install openssh-serve配置环境 /etc/ssh/ssh_config 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# This is the ssh client system-wide configuration file. See# ssh_config(5) for more information. This file provides defaults for# users, and the values can be changed in per-user configuration files# or on the command line.# Configuration data is parsed as follows:# 1. command line options# 2. user-specific file# 3. system-wide file# Any configuration value is only changed the first time it is set.# Thus, host-specific definitions should be at the beginning of the# configuration file, and defaults at the end.# Site-wide defaults for some commonly used options. For a comprehensive# list of available options, their meanings and defaults, please see the# ssh_config(5) man page.Host *# ForwardAgent no# ForwardX11 no# ForwardX11Trusted yes# RhostsRSAAuthentication no# RSAAuthentication yes PasswordAuthentication yes# HostbasedAuthentication no# GSSAPIAuthentication no# GSSAPIDelegateCredentials no# GSSAPIKeyExchange no# GSSAPITrustDNS no# BatchMode no# CheckHostIP yes# AddressFamily any# ConnectTimeout 0# StrictHostKeyChecking ask# IdentityFile ~/.ssh/identity# IdentityFile ~/.ssh/id_rsa# IdentityFile ~/.ssh/id_dsa# IdentityFile ~/.ssh/id_ecdsa# IdentityFile ~/.ssh/id_ed25519 Port 22 Protocol 2# Cipher 3des# Ciphers aes128-ctr,aes192-ctr,aes256-ctr,arcfour256,arcfour128,aes128-cbc,3des-cbc# MACs hmac-md5,hmac-sha1,umac-64@openssh.com,hmac-ripemd160# EscapeChar ~# Tunnel no# TunnelDevice any:any# PermitLocalCommand no# VisualHostKey no# ProxyCommand ssh -q -W %h:%p gateway.example.com# RekeyLimit 1G 1h SendEnv LANG LC_* HashKnownHosts yes GSSAPIAuthentication yes GSSAPIDelegateCredentials no /etc/ssh/sshd_config将PasswordAuthentication改成yes，将PubkeyAuthentication改成yes，然后保存配置文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788# Package generated configuration file# See the sshd_config(5) manpage for details# What ports, IPs and protocols we listen forPort 22# Use these options to restrict which interfaces/protocols sshd will bind to#ListenAddress ::#ListenAddress 0.0.0.0Protocol 2# HostKeys for protocol version 2HostKey /etc/ssh/ssh_host_rsa_keyHostKey /etc/ssh/ssh_host_dsa_keyHostKey /etc/ssh/ssh_host_ecdsa_keyHostKey /etc/ssh/ssh_host_ed25519_key#Privilege Separation is turned on for securityUsePrivilegeSeparation yes# Lifetime and size of ephemeral version 1 server keyKeyRegenerationInterval 3600ServerKeyBits 1024# LoggingSyslogFacility AUTHLogLevel INFO# Authentication:LoginGraceTime 120PermitRootLogin prohibit-passwordStrictModes yesRSAAuthentication yesPubkeyAuthentication yesAuthorizedKeysFile %h/.ssh/authorized_keys# Don&apos;t read the user&apos;s ~/.rhosts and ~/.shosts filesIgnoreRhosts yes# For this to work you will also need host keys in /etc/ssh_known_hostsRhostsRSAAuthentication no# similar for protocol version 2HostbasedAuthentication no# Uncomment if you don&apos;t trust ~/.ssh/known_hosts for RhostsRSAAuthentication#IgnoreUserKnownHosts yes# To enable empty passwords, change to yes (NOT RECOMMENDED)PermitEmptyPasswords no# Change to yes to enable challenge-response passwords (beware issues with# some PAM modules and threads)ChallengeResponseAuthentication no# Change to no to disable tunnelled clear text passwordsPasswordAuthentication yes# Kerberos options#KerberosAuthentication no#KerberosGetAFSToken no#KerberosOrLocalPasswd yes#KerberosTicketCleanup yes# GSSAPI options#GSSAPIAuthentication no#GSSAPICleanupCredentials yesX11Forwarding yesX11DisplayOffset 10PrintMotd noPrintLastLog yesTCPKeepAlive yes#UseLogin no#MaxStartups 10:30:60#Banner /etc/issue.net# Allow client to pass locale environment variablesAcceptEnv LANG LC_*Subsystem sftp /usr/lib/openssh/sftp-server# Set this to &apos;yes&apos; to enable PAM authentication, account processing,# and session processing. If this is enabled, PAM authentication will# be allowed through the ChallengeResponseAuthentication and# PasswordAuthentication. Depending on your PAM configuration,# PAM authentication via ChallengeResponseAuthentication may bypass# the setting of &quot;PermitRootLogin without-password&quot;.# If you just want the PAM account and session checks to run without# PAM authentication, then enable this but set PasswordAuthentication# and ChallengeResponseAuthentication to &apos;no&apos;.UsePAM yes 配置过程参考官方网站在三台虚拟机 czx-hadoop用户下,将其余两方的id_rsa.pub 放入authorized_keys，一开始都先进行 ssh localhost看是否可以免密登录1234mkdir ~/.sshchmod 700 ~/.sshssh-keygen -t rsa # id_rsa.pub、id_rsacat id_rsa.pub &gt;&gt; authorized_keys 4. 配置jdk hadoop运行环境下载jdk,解压到指定目录 /usr/lib/jvm ，hadoop可以直接指定到 ~/hadoop 当前用户的家目录下可通过用户环境变量~/.bashrc 或全局环境变量 /etc/profile 增加一下环境12345678#javaexport JAVA_HOME=/usr/lib/jvm/java export JRE_HOME=$&#123;JAVA_HOME&#125;/jre export CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib export PATH=$&#123;JAVA_HOME&#125;/bin:$PATH #hadoopexport HADOOP_INSTALL=/home/hadoop/hadoopexport PATH=$PATH:$HADOOP_INSTALL/bin 5. 设置hadoop配置文件hadoop官网参看 core-site.xml1234567891011121314151617&lt;?xml version="1.0"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;property&gt; &lt;!-- namenode RPC交互端口 --&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;final&gt;true&lt;/final&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 临时目录设定 --&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/hadoop/tmp&lt;/value&gt; &lt;description&gt;temp dir&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml12345678910111213141516171819202122232425&lt;?xml version="1.0"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;property&gt; &lt;!--存贮在本地的名字节点数据镜象的目录,作为名字节点的冗余备份--&gt; &lt;name&gt;dfs.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/hadoop/name&lt;/value&gt; &lt;final&gt;true&lt;/final&gt; &lt;/property&gt; &lt;property&gt; &lt;!--数据节点的块本地存放目录--&gt; &lt;name&gt;dfs.data.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/hadoop/data&lt;/value&gt; &lt;final&gt;true&lt;/final&gt; &lt;/property&gt; &lt;property&gt; &lt;!--缺省的块复制数量--&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;final&gt;true&lt;/final&gt; &lt;/property&gt;&lt;/configuration&gt; mared-site.xml12345678910&lt;?xml version="1.0"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;property&gt; &lt;!--作业跟踪管理器是否和MR任务在一个进程中--&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;192.168.149.135:9001&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; masters 和 slaves 再在hadoop-env.sh设置jdk路径 6. 通过ssh将配置同步到各节点1234567# 将hadoop的文件复制到其他节点上scp -r ./hadoop slave1:~scp -r ./hadoop slave2:~#将配置文件也一起复制过去,也可以手动配置，防止你的环境配置有其他配置项scp -r ~/.bashrc slave1:~scp -r ~/.bashrc slave2:~ 7. 格式化namenode12# 这一步在主结点 master 上进行操作:hadoop namenode -format 8. 启动hadoop，jps和网站检查运行情况1234#hadoop1.2.1启动，因为实验环境可能低版本比较，最新版用其他方式启动。start-all.sh jps #查看进程状态#网站查看运行状态 http://master:50030 http://master:50070 master换成master机器的ip 其他1. HDFS常用操作hadoopdfs -ls 列出HDFS下的文件hadoop dfs -ls in 列出HDFS下某个文档中的文件hadoop dfs -put test1.txt test 上传文件到指定目录并且重新命名，只有所有的DataNode都接收完数据才算成功hadoop dfs -get in getin 从HDFS获取文件并且重新命名为getin，同put一样可操作文件也可操作目录hadoop dfs -rmr out 删除指定文件从HDFS上hadoop dfs -cat in/ 查看HDFS上in目录的内容hadoop dfsadmin -report 查看HDFS的基本统计信息，结果如下hadoop dfsadmin -safemode leave 退出安全模式hadoop dfsadmin -safemode enter *进入安全模式 2. 添加节点可扩展性是HDFS的一个重要特性，首先在新加的节点上安装hadoop，然后修改$HADOOP_HOME/conf/master文件，加入 NameNode主机名，然后在NameNode节点上修改$HADOOP_HOME/conf/slaves文件，加入新加节点主机名，再建立到新加节点无密码的SSH连接 3. shell自动安装脚本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122#!/bin/bash #validate user or group validate() &#123; if [ 'id -u' == 0 ];then echo "must not be root!" exit 0 else echo "---------welcome to hadoop---------" fi &#125; #hadoop install hd-dir() &#123; if [ ! -d /home/hadoop/ ];then mkdir /home/hadoop/ else echo "download hadoop will begin" fi &#125; download-hd() &#123; wget -c http://archive.apache.org/dist/hadoop/core/hadoop-1.2.1.tar.gz -O /home/hadoop/hadoop-1.2.1.tar.gz tar -xzvf /home/hadoop/hadoop-1.2.1.tar.gz -C /home/hadoop rm /home/hadoop/hadoop-1.2.1.tar.gz Ln -s /home/hadoop/hadoop-1.2.1 /home/hadoop/hadoop1.2.1 &#125; download-java() &#123; wget -c wget -c http://download.oracle.com/otn-pub/java/jdk/7/jdk-7-linux-i586.tar.gz -O /home/hadoop/jdk-7-linux-i586.tar.gz if [ ! -d /usr/lib/jvm ];then mkdir /usr/lib/jvm else tar -xzvf /home/hadoop/jdk-7-linux-i586.tar.gz -C /usr/lib/jvm rm /home/hadoop/jdk-7-linux-i586.tar.gz fi &#125; #hadoop conf hd-conf() &#123; echo "export JAVA_HOME=/usr/lib/jvm/java" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/hadoop-env.sh echo "#set path jdk" &gt;&gt; /home/hadoop/.bashrc echo "export JAVA_HOME=/usr/lib/jvm/java" &gt;&gt; /home/hadoop/.bashrc echo "#hadoop path" &gt;&gt; /home/hadoop/.bashrc echo "export HADOOP_HOME=/home/hadoop/hadoop1.2.1" &gt;&gt; /home/hadoop/.bashrc echo "PATH=$PATH:$HADOOP_HOME/bin:$JAVA_HOME/bin" &gt;&gt; /home/hadoop/.bashrc echo "HADOOP_HOME_WARN_SUPPRESS=1" &gt;&gt; /home/hadoop/.bashrc #hadoop core-site.xml echo "&lt;configuration&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/core-site.xml echo "&lt;property&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/core-site.xml echo "&lt;name&gt;fs.default.name&lt;/name&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/core-site.xml echo "&lt;value&gt;hdfs://master:9000" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/core-site.xml echo "&lt;/property&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/core-site.xml echo "&lt;property&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/core-site.xml echo "&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/core-site.xml echo "&lt;value&gt;/home/hadoop/tmp&lt;/value&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/core-site.xml echo "&lt;/property&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/core-site.xml echo "&lt;/configuration&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/core-site.xml #hadoop hdfs-site.xml echo "&lt;configuration&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/hdfs-site.xml echo "&lt;property&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/hdfs-site.xml echo "&lt;name&gt;dfs.name.dir&lt;/name&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/hdfs-site.xml echo "&lt;value&gt;/home/hadoop/name&lt;/value&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/hdfs-site.xml echo "&lt;/property&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/hdfs-site.xml echo "&lt;property&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/hdfs-site.xml echo "&lt;name&gt;dfs.data.dir&lt;/name&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/hdfs-site.xml echo "&lt;value&gt;/home/hadoop/data&lt;/value&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/hdfs-site.xml echo "&lt;/property&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/hdfs-site.xml echo "&lt;property&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/hdfs-site.xml echo "&lt;name&gt;dfs.replication&lt;/name&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/hdfs-site.xml echo "&lt;value&gt;1&lt;/value&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/hdfs-site.xml echo "&lt;/property&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/hdfs-site.xml echo "&lt;/configuration&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/hdfs-site.xml # hadoop mapred-site.xml echo "&lt;configuration&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/mapred-site.xml echo "&lt;property&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/mapred-site.xml echo "&lt;name&gt;mapred.job.tracker&lt;/name&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/mapred-site.xml echo "&lt;value&gt;master:9001&lt;/value&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/mapred-site.xml echo "&lt;/property&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/mapred-site.xml echo "&lt;/configuration&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/mapred-site.xml #hadoop master echo "hadoop-master" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/masters #hadoop slaves echo "hadoop-master" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/slaves source /home/hadoop/.bashrc &#125; hd-start() &#123; hadoop namenode -format &#125; yes-or-no() &#123; echo "Is your name $* ?" while true do echo -n "Enter yes or no: " read x case "$x" in y | yes ) return 0;; n | no ) return 1;; * ) echo "Answer yes or no";; esac done &#125; echo "Original params are $*" if yes-or-no "$1" then echo "HI $1,nice name!" validate hd-dir download-hd download-java hd-conf else echo "Never mind!" fi]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[honeyc's Yst]]></title>
      <url>%2F2016%2F11%2F26%2Fhoney%2F</url>
      <content type="text"><![CDATA[Dillinger Dillinger is a cloud-enabled, mobile-ready, offline-storage, AngularJS powered HTML5 Markdown editor. Type some Markdown on the left See HTML in the right Magic You can also: Import and save files from GitHub, Dropbox, Google Drive and One Drive Drag and drop files into Dillinger Export documents as Markdown, HTML and PDF Markdown is a lightweight markup language based on the formatting conventions that people naturally use in email. As John Gruber writes on the Markdown site The overriding design goal for Markdown’sformatting syntax is to make it as readableas possible. The idea is that aMarkdown-formatted document should bepublishable as-is, as plain text, withoutlooking like it’s been marked up with tagsor formatting instructions. This text you see here is actually written in Markdown! To get a feel for Markdown’s syntax, type some text into the left window and watch the results in the right. TechDillinger uses a number of open source projects to work properly: AngularJS - HTML enhanced for web apps! Ace Editor - awesome web-based text editor markdown-it - Markdown parser done right. Fast and easy to extend. Twitter Bootstrap - great UI boilerplate for modern web apps node.js - evented I/O for the backend Express - fast node.js network app framework @tjholowaychuk Gulp - the streaming build system keymaster.js - awesome keyboard handler lib by @thomasfuchs jQuery - duh And of course Dillinger itself is open source with a public repository on GitHub. InstallationDillinger requires Node.js v4+ to run. Download and extract the latest pre-built release. Install the dependencies and devDependencies and start the server. 123$ cd dillinger$ npm install -d$ node app For production environments… 123$ npm install --production$ npm run predeploy$ NODE_ENV=production node app PluginsDillinger is currently extended with the following plugins Dropbox Github Google Drive OneDrive Readmes, how to use them in your own application can be found here: plugins/dropbox/README.md plugins/github/README.md plugins/googledrive/README.md plugins/onedrive/README.md DevelopmentWant to contribute? Great! Dillinger uses Gulp + Webpack for fast developing.Make a change in your file and instantanously see your updates! Open your favorite Terminal and run these commands. First Tab:1$ node app Second Tab:1$ gulp watch (optional) Third:1$ karma start Building for sourceFor production release:1$ gulp build --prod Generating pre-built zip archives for distribution:1$ gulp build dist --prod DockerDillinger is very easy to install and deploy in a Docker container. By default, the Docker will expose port 80, so change this within the Dockerfile if necessary. When ready, simply use the Dockerfile to build the image. 12cd dillingernpm run-script build-docker This will create the dillinger image and pull in the necessary dependencies. Moreover, this uses a hack to get a more optimized npm build by copying the dependencies over and only installing when the package.json itself has changed. Look inside the package.json and the Dockerfile for more details on how this works. Once done, run the Docker image and map the port to whatever you wish on your host. In this example, we simply map port 8000 of the host to port 80 of the Docker (or whatever port was exposed in the Dockerfile): 1docker run -d -p 8000:8080 --restart="always" &lt;youruser&gt;/dillinger:latest Verify the deployment by navigating to your server address in your preferred browser. 1127.0.0.1:8000 Kubernetes + Google CloudSee KUBERNETES.md docker-compose.ymlChange the path for the nginx conf mounting path to your full path, not mine! N|Solid and NGINXMore details coming soon. Todos Write Tests Rethink Github Save Add Code Comments Add Night Mode LicenseMIT Free Software, Hell Yeah!]]></content>
    </entry>

    
  
  
</search>
