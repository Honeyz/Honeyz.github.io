<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[mongodb-1]]></title>
      <url>%2F2016%2F12%2F15%2Fmongodb-1%2F</url>
      <content type="text"><![CDATA[mongodb支持google提出的mapreduce并行编程，通过启动分片和水平扩展、读写分离，与关系数据库比较：table –&gt; collection; row –&gt;doucument/object(BSON)。 mongodb进程 mongod进程：负责整个mongodb中最核心的内容，负责数据库的创建， 删除等，运行在服务器上进行监听。 1234567mongod --config /home/honeycc/properties.conf# properties.conf# dbpath = /home/honeycc/db/data# logpath = /home/honeycc/logs/log.log# journal = true 启动数据库实例的日志功能，数据库当宕机后重启恢复# port = 27017 # auth = true 启动数据库实例的权限控制功能 mongo进程:交互式shell界面 1mongo --port 27017 -username root -password root -autheticationDatabase gps_db mongodump备份、mongoexport、mongoimport、mongos分片、mongofiles分布式文件存储系统接口、mongostat状态、mongotop性能跟踪 12345mongodump --port 27017 --db gps_db --out /home/honeycc/backmongoexport --port 27017 --db gps_db --collection gps_data --out /home/honeycc/gps_data.jsonmongoinport --port 27017 --db gps_db --collection gps_data --file /home/honeycc/gps_data.json mongodb 数据库操作 查询选择器 123456789101112131415161718db.gps_data.find().pretty() # 格式化输出db.gps_data.find(&#123;id:9,&quot;province&quot;:&quot;Zhejiang&quot;&#125;) # 精确查找db.gps_data.find(&#123;&quot;GPSTIME&quot;:&#123;&quot;&amp;lt&quot;:new Date(&quot;2016-12-10&quot;)&#125;)# 比较操作符&quot;$gt&quot; 、&quot;$gte&quot;、 &quot;$lt&quot;、 &quot;$lte&quot;、&quot;$ne&quot;(分别对应&quot;&gt;&quot;、 &quot;&gt;=&quot; 、&quot;&lt;&quot; 、&quot;&lt;=&quot;、&quot;=&quot;)，组合起来进行范围的查找db.gps_data.find(&#123;&quot;GPSTIME&quot;:&#123;&quot;$in&quot;:[null],$exists:true&#125;&#125;)# 返回gpstime为null值的文档，如果$exists:false返回gpstime不为null的所有字段#$in:[] ; $nin:[].单独使用$nin $ne会进行全表扫描,应与其他选择器配合使用 db.gps_data.find(&#123;$and:[&#123;&#125;,&#123;&#125;]&#125;)# $or:[&#123;&#125;,&#123;&#125;] $and:[&#123;&#125;,&#123;&#125;]db.gps_data.find(&quot;driver.message.age&quot;:20)# 多层结构 查询投射 123456789db.gps_data.find(&#123;&quot;GPSTIME&quot;:&#123;&quot;&amp;lt&quot;:new Date(&quot;2016-12-10&quot;)&#125;,&#123;_id:0,GPSTIME:1,CODE:1,driver.message.phone:1,GPSTIME:&#123;&quot;$slice&quot;:-1&#125;&#125;)# db.gps_data.find(&#123;&#125;,&#123;&#125;)# 第一个&#123;&#125;内为查询选择器;第二个&#123;&#125;内为对前面返回的结构集进一步过滤条件，投射项,$slice:-1返回数组最后一条数据db.gps_data.find(&#123;&#125;).sort(&#123;id:-1&#125;)# 排序效率低下，尽量确保排序的在索引上db.gps_data.find(&#123;&#125;).skip(10).limit(5).sort(&#123;id:-1&#125;)# 先排序，然后跳过10条，获取5条，尽量少用skip，效率低 数组操作 12345678db.gps_data.find(&#123;driver.message.phone:[110,112]&#125;)# 数组精确匹配db.gps_data.find(&#123;driver.message.phone:112&#125;)# 数组包含匹配 db.gps_data.find(&#123;driver.message.phone.1:112&#125;)# 数组位置匹配，第二个电话为112的 增删改 123456789101112131415db.gps_data.insert(&#123;&#125;) # _id 要唯一 db.gps_data.update(query,update,&lt;upsert&gt;,&lt;multi&gt;)db.gps_data.update(&#123;CODE:&quot;浙B26827&quot;&#125;,&#123;$set : &#123; GPSTIME:new Date() &#125;,$inc:&#123;driver.message.phone:100&#125;&#125;) # 第一个文档 更改车牌为浙B26827的，$set 把GPSTIME更改为当前时间，$inc 新增属性db.gps_data.update(&#123;CODE:&quot;浙B26827&quot;&#125;,&#123;CODE:&quot;浙B26821&quot;&#125;) # 第一个文档 把其他属性都擦除掉了 db.gps_data.update(&#123;CODE:&quot;浙B26827&quot;&#125;,&#123;$set:&#123;GPSTIME:new Date() &#125;,$inc:&#123;driver.message.phone:100&#125;&#125;,&#123;upsert:true&#125; ，&#123;multi:true&#125;) # &#123;multi:true&#125; 所有文档 &#123;upsert:true&#125; 找不到则插入db.gps_data.remove(&lt;query&gt;,&lt;justOne&gt;)db.gps_data.remove(&#123;CODE:&quot;浙B26827&quot;&#125;,1)# 删除全部、limt 1 删除一个 索引:提高数据获取的性能，在linux文件系统中，磁盘抽象为 引导块 超级块 索引节点表 数据块 流程:索引节点表保存了所有文件或者目录对应的inode节点，通过文件名或目录找到对应的inode节点，通过inode节点定位到文件数据在文件系统中的逻辑块号，最后根据磁盘驱动程序将逻辑块号映射到磁盘具体的块号。 数据库上保存记录的机制是建立在文件系统上的，索引也是以文件的形式存储在磁盘上，运用最多的索引结构是B树。 Mongodb索引:B+树db.gps_data.ensureIndex({})创建索引db.gps_data.dropIndex(“GPSTIME_1_CODE_1 “) 单字段索引 _id，唯一索引 {unique:true}: db.system.indexes.find() 符合索引 db.gps_data.ensureIndex({GPSTIME:1,CODE:1}) 数组的多键索引 对一个值为数组类型的字段创建索引，则默认会对数组中的每一个元素创建索引。 聚集分析:聚集操作是对数据进行分析的有效手段。mongodb主要提供了三种对数据进行分析的计算方式:管道模式聚集分析、Mapreduce聚集分析、简单函数和命令的聚集分析 管道模式:类unix上的管道命令 grep 层层过滤 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869 db.gps_data.aggregate([&#123;$match:&#123;&#125;&#125;,&#123;$group:&#123;&#125;&#125;]) &apos;&apos;&apos; 常用管道操作符 $match: 过滤文档，值传递匹配文档到管道中的下一步骤 $limit: 限制管道中文档的数量 $skip: 跳过指定数量文档 $sort: 多所有文档进行排序 $group: 对所有文档进行分组然后计算聚集结果 $out: 将管道中的文档输出到一个具体集合中 与$group操作一起使用的计算聚集值的操作符 $first、$last、$max、$min、$avg、$sum &apos;&apos;&apos; # SQL语句与mongodb聚集操作语句比较 # select count(*) as count from gps_data db.gps_data.aggregate([ &#123;$group:&#123;_id:null,count:&#123;$sum:1&#125;&#125;&#125; ]) # select sum(num) as total from gps_data db.gps_data.aggregate([ &#123;$group:&#123;_id:null,total:&#123;$sum:&quot;$num&quot;&#125;&#125;&#125; ]) # select driver_id,sum(num) as total from gps_data group by driver_id db.gps_data.aggregate([ &#123;$group:&#123;_id:&quot;$driver_id&quot;,total:&#123;$sum:&quot;$num&quot;&#125;&#125;&#125; ]) # select driver_id,gpsdate,sum(num) as total from gps_data group by driver_id db.gps_data.aggregate([ &#123;$group:&#123;_id:&#123;driver_id:&quot;$driver_id&quot;,gpsdate:&quot;gpsdate&quot;&#125;,total:&#123;$sum:&quot;$num&quot;&#125;&#125;&#125; ]) # select driver_id, count(*) from gps_data group by driver_id having count(*) &gt; 1 db.gps_data.aggregate([ &#123;$group:&#123;_id:&quot;$driver_id&quot;,count:&#123;$sum:1&#125;&#125;&#125;, &#123;$match:&#123;count :&#123;$gt:1&#125;&#125;&#125; ])``` 2. Mapreduce模式聚集:用Mongodb做分布式存储，然后再用Mapreduce来做分析```shell db.gps_data.mapReduce( # map 函数 function()&#123; emit(this.driver_id,this.num); &#125;, # reduce 函数 funtion(key,values)&#123; return Array.sum(values) &#125;, &#123; query:&#123; CODE:&quot;浙B25681&quot;&#125;， # 查询条件 outresult:&quot;driver_car_total&quot; # 输出结果到集合上 &#125; ) &lt;----&gt; select sum(num) as value, driver_id as _id from gps_data where CODE=&quot;浙B25681&quot; group by driver_id 将结果输出到 driver_car_total集合上 db.driver_cat_total.find() &#123; &quot;_id&quot;:1,&quot;value&quot;:2 &#125; 简单聚集函数 12345678910111213141516# 1、distinct# db.collection.distinct(key,&lt;query&gt;)# 2、count# db.collection.find(&lt;query&gt;).count()# 3、group，结果集不能大于16M，不能在分片集群上进行操作且不能处理超过10000个唯一键值# db.collection.group(&#123;key:..., initial:...,reduce:...[,cond:...]&#125;)# eg:db.gps_data.group(&#123; key:&#123;_id:1&#125;, cond:&#123;_id:&#123;$lt:3&#125;&#125;, reduce:function(cur,result)&#123; result.count += cur.count; &#125; initial: &#123;count:0&#125;&#125;)# 统计_id小于3,按照_id分组求value值的和 JAVA操作Mongodb Maven pom.xml配置驱动 12345 &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;mongo-java-driver&lt;/artifactId&gt; &lt;version&gt;3.4.0&lt;/version&gt;&lt;/dependency&gt; Mongodb工具类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193public class MongoDBUtil &#123; private final static ThreadLocal&lt;Mongo&gt; mongos = new ThreadLocal&lt;Mongo&gt;(); private static final String MONGODB_ADDRESS = "127.0.0.1"; private static final int MONGODB_PORT = 27017; private static final String MONGODB_DBNAME = "gps_db";public static DB getdb()&#123; return getMongos().getDB(MONGODB_DBNAME);&#125;public static Mongo getMongos() &#123; Mongo mongo = mongos.get(); if (mongo == null) &#123; try &#123; mongo = new Mongo(MONGODB_ADDRESS,MONGODB_PORT); mongos.set(mongo); &#125; catch (MongoException e) &#123; e.printStackTrace(); &#125; &#125; return mongo;&#125;public static void close()&#123; Mongo mongo = mongos.get(); if(mongo!=null)&#123; mongo.close(); mongos.remove(); &#125;&#125;/** * 获取集合（表） * * @param collection */public static DBCollection getCollection(String collection) &#123; return getdb().getCollection(collection);&#125;/** * 插入 * * @param collection * @param o 插入 * */public static void insert(String collection, DBObject o) &#123; getCollection(collection).insert(o);&#125;/** * 批量插入 * * @param collection * @param list * 插入的列表 */public void insertBatch(String collection, List&lt;DBObject&gt; list) &#123; if (list == null || list.isEmpty()) &#123; return; &#125; getCollection(collection).insert(list);&#125;/** * 删除 * * @param collection * @param q * 查询条件 */public void delete(String collection, DBObject q) &#123; getCollection(collection).remove(q);&#125;/** * 批量删除 * * @param collection * @param list * 删除条件列表 */public void deleteBatch(String collection, List&lt;DBObject&gt; list) &#123; if (list == null || list.isEmpty()) &#123; return; &#125; for (int i = 0; i &lt; list.size(); i++) &#123; getCollection(collection).remove(list.get(i)); &#125;&#125;/** * 更新 * * @param collection * @param q * 查询条件 * @param setFields * 更新对象 */public static void update(String collection, DBObject q, DBObject setFields) &#123; getCollection(collection).updateMulti(q, new BasicDBObject("$set", setFields));&#125;/** * 查找集合所有对象 * * @param collection */public static List&lt;DBObject&gt; findAll(String collection) &#123; return getCollection(collection).find().toArray();&#125;/** * 按顺序查找集合所有对象 * * @param collection * 数据集 * @param orderBy * 排序 */public static List&lt;DBObject&gt; findAll(String collection, DBObject orderBy) &#123; return getCollection(collection).find().sort(orderBy) .toArray();&#125;/** * 查找（返回一个对象） * * @param collection * @param q * 查询条件 */public static DBObject findOne(String collection, DBObject q) &#123; return getCollection(collection).findOne(q);&#125;/** * 查找（返回一个对象） * * @param collection * @param q * 查询条件 * @param fileds * 返回字段 */public static DBObject findOne(String collection, DBObject q, DBObject fileds) &#123; return getCollection(collection).findOne(q, fileds);&#125;/** * 分页查找集合对象，返回特定字段 * * @param collection * @param q * 查询条件 * @param fileds * 返回字段 * @pageNo 第n页 * @perPageCount 每页记录数 */public static List&lt;DBObject&gt; findLess(String collection, DBObject q, DBObject fileds, int pageNo, int perPageCount) &#123; return getCollection(collection).find(q, fileds) .skip((pageNo - 1) * perPageCount).limit(perPageCount) .toArray();&#125;public static long getCollectionCount(String collection) &#123; return getCollection(collection).getCount();&#125; 使用mongodb插件 mongodb plugin 12345678910111213141516 &lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;mongo-java-driver&lt;/artifactId&gt; &lt;version&gt;3.4.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.21&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.cybermkd&lt;/groupId&gt; &lt;artifactId&gt;MongodbPlugin&lt;/artifactId&gt; &lt;version&gt;1.0.8.0&lt;/version&gt;&lt;/dependency&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193 //初始化public static MongoClient init()&#123; MongoPlugin mongoPlugin=new MongoPlugin(); mongoPlugin.add(MONGODB_ADDRESS,MONGODB_PORT); mongoPlugin.setDatabase(MONGODB_DBNAME); MongoClient client = mongoPlugin.getMongoClient(); MongoKit.INSTANCE.init(client, mongoPlugin.getDatabase()); return client;&#125;//使用 public JSONArray getCount() &#123; JSONArray jsonArray = new JSONArray(); MongoClient client = null; try &#123; client = MongoDBUtil.init(); MongoQuery query = new MongoQuery(); query.use("test").ascending("GPSTIME").find(); MongoAggregation aggregation=new MongoAggregation(query); MongoQuery queryGroupBy = aggregation .group(new Document("CODE","$CODE").append("GPSDATE","$GPSDATE")) .getQuery(); List&lt;JSONObject&gt; jsonList = queryGroupBy .projection("GPSDATE","CODE","GPSTIME","country","city","province","district","LNG","LAT") .findAll(); System.out.println(jsonList.get(0)); jsonArray.add(jsonList.get(0)); return jsonArray; &#125;finally &#123; client.close(); &#125;&#125; public static void main( String args[] )&#123; try&#123; /* FindIterable&lt;Document&gt; findIterable = collection.find(); MongoCursor&lt;Document&gt; mongoCursor = findIterable.iterator(); while(mongoCursor.hasNext())&#123; System.out.println(mongoCursor.next()); &#125; DBObject dbObject = new BasicDBObject("province","浙江省"); System.out.print(MongoDBUtil.findOne("test",dbObject).toString()); // 连接到 mongodb 服务 MongoClient mongoClient = new MongoClient( "localhost" , 27017 ); // 连接到数据库 MongoDatabase mongoDatabase = mongoClient.getDatabase("test"); MongoCollection&lt;Document&gt; collection = mongoDatabase.getCollection("test"); System.out.println(collection.count()); BasicDBObject key = new BasicDBObject("CODE",true);*/ /* System.out.println(aggregation1 .group(new Document("CODE","$CODE").append("GPSDATE","$GPSDATE")) .aggregate()); query.use("test").ascending("GPSTIME"); MongoAggregation aggregation=new MongoAggregation(query); System.out.print(JSON.toJSONString( aggregation .include("CODE","GPSDATE","province") .group("$CODE",new MongoAccumulator().max("GPSTIME","$GPSTIME")) .aggregate())); System.out.print(JSON.toJSONString( aggregation .include("CODE","GPSDATE","province") .group("$CODE",new MongoAccumulator().min("GPSTIME","$GPSTIME")) .aggregate()));*/ /*JSON.toJSONString( aggregation .group("$CODE",new MongoAccumulator().max("GPSTIME","$GPSTIME")) .aggregate())*/ MongoPlugin mongoPlugin=new MongoPlugin(); mongoPlugin.add("127.0.0.1",27017); mongoPlugin.setDatabase("test"); MongoClient client = mongoPlugin.getMongoClient(); MongoKit.INSTANCE.init(client, mongoPlugin.getDatabase()); MongoQuery query = new MongoQuery(); query.use("test")./*eq("CODE","Y115浙B92236").*/ascending("GPSTIME").find(); MongoAggregation aggregation=new MongoAggregation(query); System.out.print(JSON.toJSONString(aggregation .include("GPSDATE","CODE","GPSTIME","country","city","province","district","LNG","LAT") .projection() //.pipeline(new Document("$match",new Document("provice", "浙江省"))) .pipeline(new Document("$group", new Document("_id",new Document("province", "$province") .append("GPSDATE","$GPSDATE")) .append("total",new Document("$sum",1)))) .out("output") .aggregate())); /* MongoQuery queryGroupBy = aggregation .group(new Document("CODE","$CODE").append("GPSDATE","$GPSDATE")) .getQuery(); List&lt;JSONObject&gt; jsonList = queryGroupBy .projection("GPSDATE","CODE","GPSTIME","country","city","province","district","LNG","LAT") .findAll(); System.out.println(jsonList.get(0));*/ client.close(); &#125;catch(Exception e)&#123; System.err.println( e.getClass().getName() + ": " + e.getMessage() ); &#125; public static void main( String args[] )&#123; try&#123; /* FindIterable&lt;Document&gt; findIterable = collection.find(); MongoCursor&lt;Document&gt; mongoCursor = findIterable.iterator(); while(mongoCursor.hasNext())&#123; System.out.println(mongoCursor.next()); &#125; DBObject dbObject = new BasicDBObject("province","浙江省"); System.out.print(MongoDBUtil.findOne("test",dbObject).toString()); // 连接到 mongodb 服务 MongoClient mongoClient = new MongoClient( "localhost" , 27017 ); // 连接到数据库 MongoDatabase mongoDatabase = mongoClient.getDatabase("test"); MongoCollection&lt;Document&gt; collection = mongoDatabase.getCollection("test"); System.out.println(collection.count()); BasicDBObject key = new BasicDBObject("CODE",true);*/ /* System.out.println(aggregation1 .group(new Document("CODE","$CODE").append("GPSDATE","$GPSDATE")) .aggregate()); query.use("test").ascending("GPSTIME"); MongoAggregation aggregation=new MongoAggregation(query); System.out.print(JSON.toJSONString( aggregation .include("CODE","GPSDATE","province") .group("$CODE",new MongoAccumulator().max("GPSTIME","$GPSTIME")) .aggregate())); System.out.print(JSON.toJSONString( aggregation .include("CODE","GPSDATE","province") .group("$CODE",new MongoAccumulator().min("GPSTIME","$GPSTIME")) .aggregate()));*/ /*JSON.toJSONString( aggregation .group("$CODE",new MongoAccumulator().max("GPSTIME","$GPSTIME")) .aggregate())*/ MongoPlugin mongoPlugin=new MongoPlugin(); mongoPlugin.add("127.0.0.1",27017); mongoPlugin.setDatabase("test"); MongoClient client = mongoPlugin.getMongoClient(); MongoKit.INSTANCE.init(client, mongoPlugin.getDatabase()); MongoQuery query = new MongoQuery(); query.use("test")./*eq("CODE","Y115浙B92236").*/ascending("GPSTIME").find(); MongoAggregation aggregation=new MongoAggregation(query); System.out.print(JSON.toJSONString(aggregation .include("GPSDATE","CODE","GPSTIME","country","city","province","district","LNG","LAT") .projection() //.pipeline(new Document("$match",new Document("provice", "浙江省"))) .pipeline(new Document("$group", new Document("_id",new Document("province", "$province") .append("GPSDATE","$GPSDATE")) .append("total",new Document("$sum",1)))) .out("output") .aggregate())); /* MongoQuery queryGroupBy = aggregation .group(new Document("CODE","$CODE").append("GPSDATE","$GPSDATE")) .getQuery(); List&lt;JSONObject&gt; jsonList = queryGroupBy .projection("GPSDATE","CODE","GPSTIME","country","city","province","district","LNG","LAT") .findAll(); System.out.println(jsonList.get(0));*/ client.close(); &#125;catch(Exception e)&#123; System.err.println( e.getClass().getName() + ": " + e.getMessage() ); &#125; mongodb api文档教程mongodb plugin 插件使用教程 总结*第一次接触nosql数据库，还有很多需要去学习东西，在java上去开发还不是那么流畅，插件的使用还不是很顺畅，但是运用起来还是感觉很舒服。 在数据量打的情况下，只做数据处理的前提，最好是先在数据库去将数据处理好，再用java mongodb api 去调用数据库，获取处理好的数据，避免的在逻辑层去处理大量的数据，这样的性能效率才能高*]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[mybatis-配置加载-源码分析]]></title>
      <url>%2F2016%2F12%2F08%2Fmybatis%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F</url>
      <content type="text"><![CDATA[一、Mybaits基础MyBatis 是支持定制化 SQL、存储过程以及高级映射的优秀的持久层框架。MyBatis 避免了几乎所有的 JDBC 代码和手工设置参数以及抽取结果集。MyBatis 使用简单的 XML 或注解来配置和映射基本体，将接口和 Java 的 POJOs(Plain Old Java Objects,普通的 Java对象)映射成数据库中的记录。 1.MyBatis的框架架构 API接口层：提供给外部使用的接口API，开发人员通过这些本地API来操纵数据库。接口层一接收到调用请求就会调用数据处理层来完成具体的数据处理。 数据处理层：负责具体的SQL查找、SQL解析、SQL执行和执行结果映射处理等。它主要的目的是根据调用的请求完成一次数据库操作。 基础支撑层：负责最基础的功能支撑，包括连接管理、事务管理、配置加载和缓存处理，这些都是共用的东西，将他们抽取出来作为最基础的组件。为上层的数据处理层提供最基础的支撑。 2.原理详解：MyBatis应用程序根据XML配置文件创建SqlSessionFactory，SqlSessionFactory在根据配置，配置来源于两个地方，一处是配置文件，一处是Java代码的注解，获取一个SqlSession。SqlSession包含了执行sql所需要的所有方法，可以通过SqlSession实例直接运行映射的sql语句，完成对数据的增删改查和事务提交等，用完之后关闭SqlSession。 二、 Mybaits源码结构源代码主要在org.apache.ibatis目录下，18个包，其中在应用中主要的包有：builder、session、cache、type、transaction、datasource、jdbc、mapping，提供支撑服务的包有annotation、binding、io、logging、plugin、reflection、scripting、exception、executor、parsing.1.注解org.apache.ibatis.annotations2.绑定org.apache.ibatis.binding3.构建org.apache.ibatis.builderorg.apache.ibatis.builder.annotationorg.apache.ibatis.builder.xml4.缓存org.apache.ibatis.cacheorg.apache.ibatis.cache.decoratorsorg.apache.ibatis.cache.impl5.数据源org.apache.ibatis.datasourceorg.apache.ibatis.datasource.jndiorg.apache.ibatis.datasource.pooledorg.apache.ibatis.datasource.unpooled6.异常org.apache.ibatis.exceptions7.执行器org.apache.ibatis.executororg.apache.ibatis.executor.keygenorg.apache.ibatis.executor.loaderorg.apache.ibatis.executor.loader.cgliborg.apache.ibatis.executor.loader.javassistorg.apache.ibatis.executor.parameterorg.apache.ibatis.executor.resultorg.apache.ibatis.executor.resultsetorg.apache.ibatis.executor.statement8.IOorg.apache.ibatis.io通过类加载器在jar包中寻找一个package下满足条件(比如某个接口的子类)的所有类9.jdbc单元测试工具org.apache.ibatis.jdbc10.日志org.apache.ibatis.loggingorg.apache.ibatis.logging.commonsorg.apache.ibatis.logging.jdbcorg.apache.ibatis.logging.jdk14org.apache.ibatis.logging.log4jorg.apache.ibatis.logging.log4j2org.apache.ibatis.logging.nologgingorg.apache.ibatis.logging.slf4jorg.apache.ibatis.logging.stdout11.映射org.apache.ibatis.mapping12.解析org.apache.ibatis.parsingxml解析，${} 格式的字符串解析13.插件org.apache.ibatis.plugin14.反射org.apache.ibatis.reflectionorg.apache.ibatis.reflection.factoryorg.apache.ibatis.reflection.invokerorg.apache.ibatis.reflection.propertyorg.apache.ibatis.reflection.wrapper15.脚本org.apache.ibatis.scriptingorg.apache.ibatis.scripting.defaultsorg.apache.ibatis.scripting.xmltags16.会话org.apache.ibatis.sessionorg.apache.ibatis.session.defaults17.事务org.apache.ibatis.transactionorg.apache.ibatis.transaction.jdbcorg.apache.ibatis.transaction.managed18.类型处理器org.apache.ibatis.type实现java和jdbc中的类型之间转换 整体框架理解：初始化Mybatis，所有的配置都在Configuration对象中使用Mybatis，从SqlSessionFactory工厂中获取SqlSession，从Configuration对象中获取mapper对象，并返回结果Mybatis在加载mapper的时候对mapper接口的注解进行解析重要的几个包：io、session、builder、mapper（annotations、binding）、executor。 MyBatis sqlSession的产生过程(1)、SqlSessionFactoryBuilder：build方法创建SqlSessionFactory实例。是用过即丢，其生命周期只存在于方法体内。 (2)、SqlSessionFactory：openSession创建SqlSession实例的工厂。单例，存在于整合应用运行时。 (3)、SqlSession：用于执行持久化操作的对象，类似于jdbc中的Connection。 123456789101112// 通过对sqlSession的创建的例子，对源码的进行解读：public static SqlSession getSqlSession() &#123; SqlSession session = null; try &#123; InputStream stream = Resources.getResourceAsStream("sqlMapper.xml"); SqlSessionFactory factory = new SqlSessionFactoryBuilder().build(stream); session = factory.openSession(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return session; &#125; (1)SqlSessionFactoryBuilder:支持9种构造方法，其实最主要的是包含Configuration对象的构造方法，目的是为了通过加载配置文件创造SqlSessionFactory对象，真实最终返回的是DefaultSqlSessionFactory对象:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100package org.apache.ibatis.session;import java.io.IOException;import java.io.InputStream;import java.io.Reader;import java.util.Properties;import org.apache.ibatis.builder.xml.XMLConfigBuilder;import org.apache.ibatis.exceptions.ExceptionFactory;import org.apache.ibatis.executor.ErrorContext;import org.apache.ibatis.session.defaults.DefaultSqlSessionFactory; /** * Builder是一种设计模式(建造者模式),一般用于创建复杂对象 * 这里重载了9个build方法，为了提供尽可能多的方式来灵活的生成SqlSessionFactory的实例 * ，SqlSessionFactory本身不复杂*，复杂的其内部的Configuration对象 * 解析配置的过程都交给个XMLConfigBuilder，然后最终实例化是 &#123;@link #build(Configuration)&#125; 来完成的 */ public class SqlSessionFactoryBuilder &#123; //以下3个方法都是调用下面第4种方法 public SqlSessionFactory build(Reader reader) &#123; return build(reader, null, null); &#125; public SqlSessionFactory build(Reader reader, String environment) &#123; return build(reader, environment, null); &#125; public SqlSessionFactory build(Reader reader, Properties properties) &#123; return build(reader, null, properties); &#125; /* 第4种方法是最常用的，它使用了一个参照了XML文档或更特定的SqlMapConfig.xml文件的Reader实例。 可选的参数是environment和properties。Environment决定加载哪种环境(开发环境/生产环境)，包括数据源和事务管理器。 如果使用properties，那么就会加载那些properties（属性配置文件）， 那些属性可以用$&#123;propName&#125;语法形式多次用在配置文件中。 和Spring很像，一个思想。 */public SqlSessionFactory build(Reader reader, String environment, Properties properties) &#123; try &#123; //委托XMLConfigBuilder来解析xml文件，并构建 XMLConfigBuilder parser = new XMLConfigBuilder(reader, environment, properties); return build(parser.parse()); &#125; catch (Exception e) &#123; //这里是捕获异常，包装成自己的异常并抛出的idiom？，最后还要reset ErrorContext throw ExceptionFactory.wrapException("Error building SqlSession.", e); &#125; finally &#123; ErrorContext.instance().reset(); try &#123; reader.close(); &#125; catch (IOException e) &#123; // Intentionally ignore. Prefer previous error. &#125; &#125;&#125; //以下3个方法都是调用下面第8种方法 public SqlSessionFactory build(InputStream inputStream) &#123; return build(inputStream, null, null); &#125; public SqlSessionFactory build(InputStream inputStream, String environment) &#123; return build(inputStream, environment, null); &#125; public SqlSessionFactory build(InputStream inputStream, Properties properties) &#123; return build(inputStream, null, properties); &#125; /** * InputStream读取配置的最终入口 */ public SqlSessionFactory build(InputStream inputStream, String environment, Properties properties) &#123; try &#123; XMLConfigBuilder parser = new XMLConfigBuilder(inputStream, environment, properties); return build(parser.parse()); &#125; catch (Exception e) &#123; throw ExceptionFactory.wrapException("Error building SqlSession.", e); &#125; finally &#123; ErrorContext.instance().reset(); try &#123; inputStream.close(); &#125; catch (IOException e) &#123; // Intentionally ignore. Prefer previous error. &#125; &#125; &#125; /** * 创建了默认的SqlSessionFactory实现类 * @param config &#123;@link Configuration&#125;实例 * @return SqlSessionFactory的实现类：&#123;@link DefaultSqlSessionFactory&#125; */ public SqlSessionFactory build(Configuration config) &#123; return new DefaultSqlSessionFactory(config); &#125;&#125; 从中我们看到了有一段很核心的代码，下面那段，这应该是去解析XML配置文件去了，解析完得出了一个Configuration对象，可以先猜测下这个Configuration对象和我们配置文件都是一一对应的：123XMLConfigBuilder parser = new XMLConfigBuilder(inputStream, environment, properties); //此处返回的是Configuration对象 return build(parser.parse()); 再看看Configuration对象的一些核心属性：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271package org.apache.ibatis.session;import java.util.Arrays;import java.util.Collection;import java.util.HashMap;import java.util.HashSet;import java.util.LinkedList;import java.util.List;import java.util.Map;import java.util.Properties;import java.util.Set;import org.apache.ibatis.binding.MapperRegistry;import org.apache.ibatis.builder.CacheRefResolver;import org.apache.ibatis.builder.ResultMapResolver;import org.apache.ibatis.builder.annotation.MethodResolver;import org.apache.ibatis.builder.xml.XMLStatementBuilder;import org.apache.ibatis.cache.Cache;import org.apache.ibatis.cache.decorators.FifoCache;import org.apache.ibatis.cache.decorators.LruCache;import org.apache.ibatis.cache.decorators.SoftCache;import org.apache.ibatis.cache.decorators.WeakCache;import org.apache.ibatis.cache.impl.PerpetualCache;import org.apache.ibatis.datasource.jndi.JndiDataSourceFactory;import org.apache.ibatis.datasource.pooled.PooledDataSourceFactory;import org.apache.ibatis.datasource.unpooled.UnpooledDataSourceFactory;import org.apache.ibatis.executor.BatchExecutor;import org.apache.ibatis.executor.CachingExecutor;import org.apache.ibatis.executor.Executor;import org.apache.ibatis.executor.ReuseExecutor;import org.apache.ibatis.executor.SimpleExecutor;import org.apache.ibatis.executor.keygen.KeyGenerator;import org.apache.ibatis.executor.loader.ProxyFactory;import org.apache.ibatis.executor.loader.cglib.CglibProxyFactory;import org.apache.ibatis.executor.loader.javassist.JavassistProxyFactory;import org.apache.ibatis.executor.parameter.ParameterHandler;import org.apache.ibatis.executor.resultset.DefaultResultSetHandler;import org.apache.ibatis.executor.resultset.ResultSetHandler;import org.apache.ibatis.executor.statement.RoutingStatementHandler;import org.apache.ibatis.executor.statement.StatementHandler;import org.apache.ibatis.logging.Log;import org.apache.ibatis.logging.LogFactory;import org.apache.ibatis.logging.commons.JakartaCommonsLoggingImpl;import org.apache.ibatis.logging.jdk14.Jdk14LoggingImpl;import org.apache.ibatis.logging.log4j.Log4jImpl;import org.apache.ibatis.logging.log4j2.Log4j2Impl;import org.apache.ibatis.logging.nologging.NoLoggingImpl;import org.apache.ibatis.logging.slf4j.Slf4jImpl;import org.apache.ibatis.logging.stdout.StdOutImpl;import org.apache.ibatis.mapping.BoundSql;import org.apache.ibatis.mapping.Environment;import org.apache.ibatis.mapping.MappedStatement;import org.apache.ibatis.mapping.ParameterMap;import org.apache.ibatis.mapping.ResultMap;import org.apache.ibatis.mapping.VendorDatabaseIdProvider;import org.apache.ibatis.parsing.XNode;import org.apache.ibatis.plugin.Interceptor;import org.apache.ibatis.plugin.InterceptorChain;import org.apache.ibatis.reflection.MetaObject;import org.apache.ibatis.reflection.factory.DefaultObjectFactory;import org.apache.ibatis.reflection.factory.ObjectFactory;import org.apache.ibatis.reflection.wrapper.DefaultObjectWrapperFactory;import org.apache.ibatis.reflection.wrapper.ObjectWrapperFactory;import org.apache.ibatis.scripting.LanguageDriver;import org.apache.ibatis.scripting.LanguageDriverRegistry;import org.apache.ibatis.scripting.defaults.RawLanguageDriver;import org.apache.ibatis.scripting.xmltags.XMLLanguageDriver;import org.apache.ibatis.transaction.Transaction;import org.apache.ibatis.transaction.jdbc.JdbcTransactionFactory;import org.apache.ibatis.transaction.managed.ManagedTransactionFactory;import org.apache.ibatis.type.JdbcType;import org.apache.ibatis.type.TypeAliasRegistry;import org.apache.ibatis.type.TypeHandlerRegistry;/** * 配置类 */public class Configuration &#123;/** * 一个environment就是一个数据源，在多个数据源的项目中，可以配置多个environment元素，然后指定不同的ID * 在测试、开发、上线不同的环境中，可以方便的切换environment */protected Environment environment; /** 是否允许在嵌套语句中使用分页（RowBounds）。一般是不建议使用mybatis的分页的 * 因为它还是查询了所有的结果集，在内存中进行计算和分页,并不是在database端进行的物理分页 * 如果数据量较大，会造成数据库和网络压力 */ protected boolean safeResultHandlerEnabled = true;protected boolean safeRowBoundsEnabled = false; /* 是否开启自动驼峰命名规则（camel case）映射，即从经典数据库列名 A_COLUMN 到经典 Java 属性名 aColumn 的类似映射。 */protected boolean mapUnderscoreToCamelCase = false; /** 当启用时，对任意延迟属性的调用会使带有延迟加载属性的对象完整加载；反之，每种属性将会按需加载。 */protected boolean aggressiveLazyLoading = true; /** *是否允许单一语句返回多结果集（需要兼容驱动）。 */protected boolean multipleResultSetsEnabled = true; /** * 允许 JDBC 支持自动生成主键，需要驱动兼容。 如果设置为 true 则这个设置强制使用自动生成主键，尽管一些驱动不能兼容但仍可正常工作（比如 Derby）。 */protected boolean useGeneratedKeys = false; /** * 使用列标签代替列名。不同的驱动在这方面会有不同的表现， 具体可参考相关驱动文档或通过测试这两种不同的模式来观察所用驱动的结果。 */protected boolean useColumnLabel = true; /** * 是否使用缓存，该配置影响的所有映射器中配置的缓存的全局开关 */protected boolean cacheEnabled = true; /*** 指定当结果集中值为 null 的时候是否调用映射对象的 setter（map 对象时为 put）方法，这对于有 Map.keySet() 依赖或 null 值初始化的时候是有用的。注意基本类型（int、boolean等）是不能设置成 null 的。 */protected boolean callSettersOnNulls = false; /*** Mybatis的日志前缀 */protected String logPrefix; /*** 指定 MyBatis 所用日志的具体实现，未指定时将自动查找 * SLF4J | LOG4J | LOG4J2 | JDK_LOGGING | COMMONS_LOGGING | STDOUT_LOGGING | NO_LOGGING * 默认值：空 */protected Class &lt;? extends Log&gt; logImpl; protected Class&lt;? extends VFS&gt; vfsImpl; /** *Mybatis的本地缓存，默认的级别是session级的，这个缓存可以防止循环引用和加速重复的查询工作 * SESSION级的会缓存一个会话中所执行的所有查询 * STATEMENT级缓存，对同一个SqlSession的不同调用将不会共享数据 * &#123;@link LocalCacheScope&#125; */protected LocalCacheScope localCacheScope = LocalCacheScope.SESSION;/** * 当没有为参数提供特定的 JDBC 类型时，为空值指定 JDBC 类型。 某些驱动需要指定列的 JDBC 类型，多数情况直接用一般类型即可，比如 NULL、VARCHAR 或 OTHER。 */protected JdbcType jdbcTypeForNull = JdbcType.OTHER; /** * 懒加载被触发的方法 */protected Set&lt;String&gt; lazyLoadTriggerMethods = new HashSet&lt;String&gt;(Arrays.asList(new String[] &#123; "equals", "clone", "hashCode", "toString" &#125;));protected Integer defaultStatementTimeout; /** * 设置超时时间，它决定驱动等待数据库响应的秒数。 */protected Integer defaultStatementTimeout;/** * 默认的SQL执行器是SIMPLE的,具体可以详见&#123;@link SimpleExecutor&#125; */protected ExecutorType defaultExecutorType = ExecutorType.SIMPLE; /** * 指定 MyBatis 应如何自动映射列到字段或属性。 * NONE 表示取消自动映射； * PARTIAL 只会自动映射没有定义嵌套结果集映射的结果集。 * FULL 会自动映射任意复杂的结果集（无论是否嵌套） */protected AutoMappingBehavior autoMappingBehavior = AutoMappingBehavior.PARTIAL; protected Properties variables = new Properties(); protected ReflectorFactory reflectorFactory = new DefaultReflectorFactory(); protected ObjectFactory objectFactory = new DefaultObjectFactory(); protected ObjectWrapperFactory objectWrapperFactory = new DefaultObjectWrapperFactory(); protected MapperRegistry mapperRegistry = new MapperRegistry(this); /** * 延迟加载的开关，在开启时，所有的关联对象都会被延迟加载。 * 在特定的关联关系中，可以通过指定fetchType来覆盖懒加载的状态 */protected boolean lazyLoadingEnabled = false; /** * 指定 Mybatis 创建具有延迟加载能力的对象所用到的代理工具。 */protected ProxyFactory proxyFactory = new JavassistProxyFactory(); // #224 Using internal Javassist instead of OGNLprotected String databaseId; protected Class&lt;?&gt; configurationFactory; /*** 拦截器链（参考责任链chain设计模式） */protected final InterceptorChain interceptorChain = new InterceptorChain(); //类型处理器注册机 数据库类型 &lt;==&gt; Java类型 的相互转换的处理 protected final TypeHandlerRegistry typeHandlerRegistry = new TypeHandlerRegistry();//类型别名注册机protected final TypeAliasRegistry typeAliasRegistry = new TypeAliasRegistry();protected final LanguageDriverRegistry languageRegistry = new LanguageDriverRegistry();//映射的语句,存在Map里protected final Map&lt;String, MappedStatement&gt; mappedStatements = new StrictMap&lt;MappedStatement&gt;("Mapped Statements collection");//缓存,存在Map里protected final Map&lt;String, Cache&gt; caches = new StrictMap&lt;Cache&gt;("Caches collection");//结果映射,存在Map里protected final Map&lt;String, ResultMap&gt; resultMaps = new StrictMap&lt;ResultMap&gt;("Result Maps collection");protected final Map&lt;String, ParameterMap&gt; parameterMaps = new StrictMap&lt;ParameterMap&gt;("Parameter Maps collection");protected final Map&lt;String, KeyGenerator&gt; keyGenerators = new StrictMap&lt;KeyGenerator&gt;("Key Generators collection");protected final Set&lt;String&gt; loadedResources = new HashSet&lt;String&gt;();protected final Map&lt;String, XNode&gt; sqlFragments = new StrictMap&lt;XNode&gt;("XML fragments parsed from previous mappers");//不完整的SQL语句protected final Collection&lt;XMLStatementBuilder&gt; incompleteStatements = new LinkedList&lt;XMLStatementBuilder&gt;();protected final Collection&lt;CacheRefResolver&gt; incompleteCacheRefs = new LinkedList&lt;CacheRefResolver&gt;();protected final Collection&lt;ResultMapResolver&gt; incompleteResultMaps = new LinkedList&lt;ResultMapResolver&gt;();protected final Collection&lt;MethodResolver&gt; incompleteMethods = new LinkedList&lt;MethodResolver&gt;(); /** * 映射保存缓存引用关系。 关键是引用绑定到另一个命名空间的缓存的命名空间，* 该值是实际缓存绑定到的命名空间。*/ protected final Map&lt;String, String&gt; cacheRefMap = new HashMap&lt;String, String&gt;();****************** (2)、返回的DefaultSqlSessionFactory​ ，可进行Sqlsession的创建，Sqlsession对应着一次数据库会话。由于数据库回话不是永久的，因此Sqlsession的生命周期也不应该是永久的，相反，在你每次访问数据库时都需要创建它（当然并不是说在Sqlsession里只能执行一次sql，你可以执行多次，当一旦关闭了Sqlsession就需要重新创建它）。创建Sqlsession的地方只有一个，那就是SqlsessionFactory的openSession方法。SqlSessionFactory：SqlSession工厂类，以工厂形式创建SqlSession对象，采用了Factory工厂设计模式123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163package org.apache.ibatis.session;import java.sql.Connection;/** * 构建SqlSession的工厂.工厂模式 */public interface SqlSessionFactory &#123; //8个方法可以用来创建SqlSession实例 SqlSession openSession(); //自动提交 SqlSession openSession(boolean autoCommit); //连接 SqlSession openSession(Connection connection); //事务隔离级别 SqlSession openSession(TransactionIsolationLevel level); //执行器的类型 SqlSession openSession(ExecutorType execType); SqlSession openSession(ExecutorType execType, boolean autoCommit); SqlSession openSession(ExecutorType execType, TransactionIsolationLevel level); SqlSession openSession(ExecutorType execType, Connection connection); Configuration getConfiguration();&#125;DefaultSqlSessionFactory实现SqlsessionFactory。对sqlSession进行创建package org.apache.ibatis.session.defaults;import java.sql.Connection;import java.sql.SQLException;import org.apache.ibatis.exceptions.ExceptionFactory;import org.apache.ibatis.executor.ErrorContext;import org.apache.ibatis.executor.Executor;import org.apache.ibatis.mapping.Environment;import org.apache.ibatis.session.Configuration;import org.apache.ibatis.session.ExecutorType;import org.apache.ibatis.session.SqlSession;import org.apache.ibatis.session.SqlSessionFactory;import org.apache.ibatis.session.TransactionIsolationLevel;import org.apache.ibatis.transaction.Transaction;import org.apache.ibatis.transaction.TransactionFactory;import org.apache.ibatis.transaction.managed.ManagedTransactionFactory;/** * 默认的SqlSessionFactory */public class DefaultSqlSessionFactory implements SqlSessionFactory &#123; private final Configuration configuration; public DefaultSqlSessionFactory(Configuration configuration) &#123; this.configuration = configuration; &#125; //最终都会调用2种方法：openSessionFromDataSource,openSessionFromConnection //以下6个方法都会调用openSessionFromDataSource @Override public SqlSession openSession() &#123; return openSessionFromDataSource(configuration.getDefaultExecutorType(), null, false); &#125; @Override public SqlSession openSession(boolean autoCommit) &#123; return openSessionFromDataSource(configuration.getDefaultExecutorType(), null, autoCommit); &#125; @Override public SqlSession openSession(ExecutorType execType) &#123; return openSessionFromDataSource(execType, null, false); &#125; @Override public SqlSession openSession(TransactionIsolationLevel level) &#123; return openSessionFromDataSource(configuration.getDefaultExecutorType(), level, false); &#125; @Override public SqlSession openSession(ExecutorType execType, TransactionIsolationLevel level) &#123; return openSessionFromDataSource(execType, level, false); &#125; @Override public SqlSession openSession(ExecutorType execType, boolean autoCommit) &#123; return openSessionFromDataSource(execType, null, autoCommit); &#125; //以下2个方法都会调用openSessionFromConnection @Override public SqlSession openSession(Connection connection) &#123; return openSessionFromConnection(configuration.getDefaultExecutorType(), connection); &#125; @Override public SqlSession openSession(ExecutorType execType, Connection connection) &#123; return openSessionFromConnection(execType, connection); &#125; @Override public Configuration getConfiguration() &#123; return configuration; &#125;private SqlSession openSessionFromDataSource(ExecutorType execType, TransactionIsolationLevel level, boolean autoCommit) &#123; Transaction tx = null; try &#123; final Environment environment = configuration.getEnvironment(); final TransactionFactory transactionFactory = getTransactionFactoryFromEnvironment(environment); //通过事务工厂来产生一个事务 tx = transactionFactory.newTransaction(environment.getDataSource(), level, autoCommit); //生成一个执行器(事务包含在执行器里) final Executor executor = configuration.newExecutor(tx, execType); //然后产生一个DefaultSqlSession return new DefaultSqlSession(configuration, executor, autoCommit); &#125; catch (Exception e) &#123; //如果打开事务出错，则关闭它 closeTransaction(tx); // may have fetched a connection so lets call close() throw ExceptionFactory.wrapException("Error opening session. Cause: " + e, e); &#125; finally &#123; //最后清空错误上下文 ErrorContext.instance().reset(); &#125;&#125; private SqlSession openSessionFromConnection(ExecutorType execType, Connection connection) &#123; try &#123; boolean autoCommit; try &#123; autoCommit = connection.getAutoCommit(); &#125; catch (SQLException e) &#123; // Failover to true, as most poor drivers // or databases won't support transactions autoCommit = true; &#125; final Environment environment = configuration.getEnvironment(); final TransactionFactory transactionFactory = getTransactionFactoryFromEnvironment(environment); final Transaction tx = transactionFactory.newTransaction(connection); final Executor executor = configuration.newExecutor(tx, execType); return new DefaultSqlSession(configuration, executor, autoCommit); &#125; catch (Exception e) &#123; throw ExceptionFactory.wrapException("Error opening session. Cause: " + e, e); &#125; finally &#123; ErrorContext.instance().reset(); &#125; &#125;private TransactionFactory getTransactionFactoryFromEnvironment(Environment environment) &#123; //如果没有配置事务工厂，则返回托管事务工厂 if (environment == null || environment.getTransactionFactory() == null) &#123; return new ManagedTransactionFactory(); &#125; return environment.getTransactionFactory();&#125;private void closeTransaction(Transaction tx) &#123; if (tx != null) &#123; try &#123; tx.close(); &#125; catch (SQLException ignore) &#123; // Intentionally ignore. Prefer previous error. &#125; &#125;&#125;&#125; 从DefaultSessionFactory中的private SqlSession openSessionFromDataSource(ExecutorType execType, TransactionIsolationLevel level, boolean autoCommit) {}方法可以看出，创建sqlsession经过了以下几个主要步骤：1) 从配置中获取Environment；2) 从Environment中取得DataSource；3) 从Environment中取得TransactionFactory；4) 从DataSource里获取数据库连接对象Connection；5) 在取得的数据库连接上创建事务对象Transaction；6) 创建Executor对象（该对象非常重要，事实上sqlsession的所有操作都是通过它完成的）；7) 创建sqlsession对象。 时序图：设计模式：工厂模式SqlSessionFactory建造模式SqlSessionFactoryBuilder单例模式LogFactory装饰模式Cache：采用装饰模式，一个个包装起来，形成一个链，典型的就是SynchronizedCache-&gt;LoggingCache-&gt;SerializedCache-&gt;LruCache-&gt;PerpetualCache，通过链起来达到功能增加(1)、 SynchronizedCache 同步缓存，防止多线程问题。核心: 加读写锁， ReadWriteLock.readLock().lock()/unlock() ReadWriteLock.writeLock().lock()/unlock()(2)、LoggingCache 日志缓存，添加功能：取缓存时打印命中率(3)、SerializedCache 序列化缓存 用途是先将对象序列化成2进制，再缓存(4)、LruCache 最近最少使用缓存，核心就是覆盖 LinkedHashMap.removeEldestEntry方法,返回true或false告诉 LinkedHashMap要不要删除此最老键值。LinkedHashMap内部其实就是每次访问或者插入一个元素都会把元素放到链表末尾，这样不经常访问的键值肯定就在链表开头啦。(5)、PerpetualCache 永久缓存，一旦存入就一直保持，内部就是一个HashMap,所有方法基本就是直接调用HashMap的方法(6)、FifoCache 先进先出缓存，内部就是一个链表，将链表开头元素（最老）移除(7)、ScheduledCache 定时调度缓存， 目的是每一小时清空一下缓存(8)、SoftCache 软引用缓存，核心是SoftReference(9)、 WeakCache 弱引用缓存，核心是WeakReference(10)、TransactionalCache 事务缓存，一次性存入多个缓存，移除多个缓存]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[idea下使用脚本在远程hadoop环境去执行和交互]]></title>
      <url>%2F2016%2F12%2F05%2Fidea%E4%B8%8B%E4%BD%BF%E7%94%A8%E8%84%9A%E6%9C%AC%E5%9C%A8%E8%BF%9C%E7%A8%8Bhadoop%E7%8E%AF%E5%A2%83%E5%8E%BB%E6%89%A7%E8%A1%8C%E5%92%8C%E4%BA%A4%E4%BA%92%2F</url>
      <content type="text"><![CDATA[在ideal下创建maven工程，编写mapreduce程序，调用虚拟机上hadoop分布式执行，避免每次都要进行打包，将文件、jar包上传到主节点master上，在主节点上再去调用 1.idea创建maven工程，目录结构如下 input 文件下放程序需要读取的文件，每个程序对应input下一个文件夹，如wordcount，words文件为com.zju.czx.WordCount 要读取的文件;output 下为程序执行完成从hdfs上放下的文件target 存放整个项目的打包生成的jarsrc 存放mapreduce的程序shell 存放执行脚本，通过脚本来对虚拟机上进行操作 pom.xml 使用的是hadoop版本1.2.1.123456789101112131415161718192021222324252627282930313233&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.zju.czx&lt;/groupId&gt; &lt;artifactId&gt;remoteHadoop&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;remoteHadoop&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;hadoop.version&gt;1.2.1&lt;/hadoop.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-core&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 2.在idea这个开发环境的机器上，与master设置ssh的免密码登录12345678# 在ssh下生成 id_rsa.pub、id_rsa ssh-keygen -t rsa # 将公钥复制到master下面 cp id_rsa.pub honeycc_ id_rsa.pub scp .ssh/honeycc_ id_rsa.pub czx-hadoop@192.168.149.135:~/.ssh/ # 在master上，将公钥添加到 authorized_keys上 cat .ssh/honeycc_ id_rsa.pub &gt;&gt; authorized_keys #设置成功 3.编写脚本，将开发环境与master进行开发run.sh1234567891011121314151617181920212223242526272829303132333435363738#!/bin/shjarname=$1main=$2dirctory=$3if [ "$#" -eq "3" ];then echo "你的jar包的名字是:$&#123;jarname&#125;" echo "你所要执行的主类:$&#123;main&#125;" echo "你所导入的文件目录:$&#123;dirctory&#125;" echo " 将 $&#123;jarname&#125; 上传到主节点的hadoop路径下 master ~/hadoop/" scp ../target/$&#123;jarname&#125; czx-hadoop@192.168.149.135:~/hadoop/ echo " 本地文件复制到主节点上 ../input/$&#123;dirctory&#125; czx-hadoop@192.168.149.135:~/input/$&#123;dirctory&#125;" scp -r ../input/$&#123;dirctory&#125; czx-hadoop@192.168.149.135:~/input/ echo "在主节点链接 hdfs 创建文件夹 ./hadoop/bin/hadoop fs -mkdir /input/$&#123;dirctory&#125;" ssh czx-hadoop@192.168.149.135 "./hadoop/bin/hadoop fs -mkdir /input/$&#123;dirctory&#125;" echo "将主节点上文件上传到hdfs上 ./hadoop/bin/hadoop fs -put ~/input/$&#123;dirctory&#125;/* /input/$&#123;dirctory&#125;/" ssh czx-hadoop@192.168.149.135 "./hadoop/bin/hadoop fs -put ~/input/$&#123;dirctory&#125;/* /input/$&#123;dirctory&#125;/" echo "删除输出目录 ./hadoop/bin/hadoop rm -r /output/$&#123;dirctory&#125;" ssh czx-hadoop@192.168.149.135 "./hadoop/bin/hadoop fs -rmr /output/$&#123;dirctory&#125;" echo " 执行$&#123;jarname&#125; ./hadoop/bin/hadoop jar hadoop/$&#123;jarname&#125; $&#123;main&#125; /input/$&#123;dirctory&#125;/* /output/$&#123;dirctory&#125;" ssh czx-hadoop@192.168.149.135 "./hadoop/bin/hadoop jar hadoop/$&#123;jarname&#125; $&#123;main&#125; /input/$&#123;dirctory&#125;/* /output/$&#123;dirctory&#125;" ssh czx-hadoop@192.168.149.135 "rm -r ~/output/$&#123;dirctory&#125;" echo "从hdfs取出执行完成的结果 ./hadoop/bin/hadoop fs -get /output/$&#123;dirctory&#125; ~/output/$&#123;dirctory&#125;" ssh czx-hadoop@192.168.149.135 "./hadoop/bin/hadoop fs -get /output/$&#123;dirctory&#125; ~/output/$&#123;dirctory&#125;" echo "将结果复制到本地" rm -r ../output/$&#123;dirctory&#125; scp -r czx-hadoop@192.168.149.135:~/output/$&#123;dirctory&#125;/ ../output/else echo "需要提供3个参数，你只提供了 $# 参数,第一个参数为jar名字，第二个为执行主函数 第三个为输入文件的文件夹名字 eg: remoteHadoop-1.0-SNAPSHOT.jar com.zju.czx.WordCount words"fi 总结这样一来就减少了每次再手动与虚拟机进行脚本，一个脚本就进行操作了。在执行上可能要按照规定好规则去执行。目录结构的创建也要按照规则去创建。不过自己使用起来还是很方便的，而且用maven对其进行打包控制，可以重复编写多个mapreduce程序，只要在执行脚本的时候设定好主类就好了。12./run.sh remoteHadoop-1.0-SNAPSHOT.jar com.zju.czx.WordCount words# jar包，主类类名，程序要读取的文件夹]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[多台虚拟机搭建hadoop环境]]></title>
      <url>%2F2016%2F11%2F28%2Fhadoop%2F</url>
      <content type="text"><![CDATA[Hadoop配置流程 修改hosts 建立hadoop运行帐号 ssh - 免密码 配置jdk hadoop运行环境 设置hadoop配置文件 通过ssh将配置同步到各节点 格式化namenode 启动hadoop，jps和网站检查运行情况 使用的是3台ubuntu系统进行hadoop环境的搭载、hadoop版本1.2.1 1.修改hosts可先进行主机hostname的修改，三台都进行设置。 /etc/hosts1234567891011127.0.0.1 localhost localhost.localdomain localhost#127.0.1.1 ubuntu192.168.149.135 master192.168.149.134 slave2 192.168.149.133 slave1 slave1.localdomain slave1::1 ip6-localhost ip6-loopbackfe00::0 ip6-localnetff00::0 ip6-mcastprefixff02::1 ip6-allnodesff02::2 ip6-allrouters 2.建立hadoop运行帐号为 hadoop 集群专门设置一个用户组及用户，用root设置添加用户的权限123456sudo groupadd hadoopsudo useradd –s /bin/bash –d /home/hadoop –m czx-hadoop –g hadoopsudo passwd czx-hadoopvim /etc/sudoersczx-hadoop ALL=(ALL) ALL #修改用户权限 3 个虚机结点均需要进行以上步骤来完成 hadoop 运行帐号的建立。 3.ssh免密码登录需要下载sudo apt-get install openssh-serve配置环境 /etc/ssh/ssh_config 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# This is the ssh client system-wide configuration file. See# ssh_config(5) for more information. This file provides defaults for# users, and the values can be changed in per-user configuration files# or on the command line.# Configuration data is parsed as follows:# 1. command line options# 2. user-specific file# 3. system-wide file# Any configuration value is only changed the first time it is set.# Thus, host-specific definitions should be at the beginning of the# configuration file, and defaults at the end.# Site-wide defaults for some commonly used options. For a comprehensive# list of available options, their meanings and defaults, please see the# ssh_config(5) man page.Host *# ForwardAgent no# ForwardX11 no# ForwardX11Trusted yes# RhostsRSAAuthentication no# RSAAuthentication yes PasswordAuthentication yes# HostbasedAuthentication no# GSSAPIAuthentication no# GSSAPIDelegateCredentials no# GSSAPIKeyExchange no# GSSAPITrustDNS no# BatchMode no# CheckHostIP yes# AddressFamily any# ConnectTimeout 0# StrictHostKeyChecking ask# IdentityFile ~/.ssh/identity# IdentityFile ~/.ssh/id_rsa# IdentityFile ~/.ssh/id_dsa# IdentityFile ~/.ssh/id_ecdsa# IdentityFile ~/.ssh/id_ed25519 Port 22 Protocol 2# Cipher 3des# Ciphers aes128-ctr,aes192-ctr,aes256-ctr,arcfour256,arcfour128,aes128-cbc,3des-cbc# MACs hmac-md5,hmac-sha1,umac-64@openssh.com,hmac-ripemd160# EscapeChar ~# Tunnel no# TunnelDevice any:any# PermitLocalCommand no# VisualHostKey no# ProxyCommand ssh -q -W %h:%p gateway.example.com# RekeyLimit 1G 1h SendEnv LANG LC_* HashKnownHosts yes GSSAPIAuthentication yes GSSAPIDelegateCredentials no /etc/ssh/sshd_config将PasswordAuthentication改成yes，将PubkeyAuthentication改成yes，然后保存配置文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788# Package generated configuration file# See the sshd_config(5) manpage for details# What ports, IPs and protocols we listen forPort 22# Use these options to restrict which interfaces/protocols sshd will bind to#ListenAddress ::#ListenAddress 0.0.0.0Protocol 2# HostKeys for protocol version 2HostKey /etc/ssh/ssh_host_rsa_keyHostKey /etc/ssh/ssh_host_dsa_keyHostKey /etc/ssh/ssh_host_ecdsa_keyHostKey /etc/ssh/ssh_host_ed25519_key#Privilege Separation is turned on for securityUsePrivilegeSeparation yes# Lifetime and size of ephemeral version 1 server keyKeyRegenerationInterval 3600ServerKeyBits 1024# LoggingSyslogFacility AUTHLogLevel INFO# Authentication:LoginGraceTime 120PermitRootLogin prohibit-passwordStrictModes yesRSAAuthentication yesPubkeyAuthentication yesAuthorizedKeysFile %h/.ssh/authorized_keys# Don&apos;t read the user&apos;s ~/.rhosts and ~/.shosts filesIgnoreRhosts yes# For this to work you will also need host keys in /etc/ssh_known_hostsRhostsRSAAuthentication no# similar for protocol version 2HostbasedAuthentication no# Uncomment if you don&apos;t trust ~/.ssh/known_hosts for RhostsRSAAuthentication#IgnoreUserKnownHosts yes# To enable empty passwords, change to yes (NOT RECOMMENDED)PermitEmptyPasswords no# Change to yes to enable challenge-response passwords (beware issues with# some PAM modules and threads)ChallengeResponseAuthentication no# Change to no to disable tunnelled clear text passwordsPasswordAuthentication yes# Kerberos options#KerberosAuthentication no#KerberosGetAFSToken no#KerberosOrLocalPasswd yes#KerberosTicketCleanup yes# GSSAPI options#GSSAPIAuthentication no#GSSAPICleanupCredentials yesX11Forwarding yesX11DisplayOffset 10PrintMotd noPrintLastLog yesTCPKeepAlive yes#UseLogin no#MaxStartups 10:30:60#Banner /etc/issue.net# Allow client to pass locale environment variablesAcceptEnv LANG LC_*Subsystem sftp /usr/lib/openssh/sftp-server# Set this to &apos;yes&apos; to enable PAM authentication, account processing,# and session processing. If this is enabled, PAM authentication will# be allowed through the ChallengeResponseAuthentication and# PasswordAuthentication. Depending on your PAM configuration,# PAM authentication via ChallengeResponseAuthentication may bypass# the setting of &quot;PermitRootLogin without-password&quot;.# If you just want the PAM account and session checks to run without# PAM authentication, then enable this but set PasswordAuthentication# and ChallengeResponseAuthentication to &apos;no&apos;.UsePAM yes 配置过程参考官方网站在三台虚拟机 czx-hadoop用户下,将其余两方的id_rsa.pub 放入authorized_keys，一开始都先进行 ssh localhost看是否可以免密登录1234mkdir ~/.sshchmod 700 ~/.sshssh-keygen -t rsa # id_rsa.pub、id_rsacat id_rsa.pub &gt;&gt; authorized_keys 4. 配置jdk hadoop运行环境下载jdk,解压到指定目录 /usr/lib/jvm ，hadoop可以直接指定到 ~/hadoop 当前用户的家目录下可通过用户环境变量~/.bashrc 或全局环境变量 /etc/profile 增加一下环境12345678#javaexport JAVA_HOME=/usr/lib/jvm/java export JRE_HOME=$&#123;JAVA_HOME&#125;/jre export CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib export PATH=$&#123;JAVA_HOME&#125;/bin:$PATH #hadoopexport HADOOP_INSTALL=/home/hadoop/hadoopexport PATH=$PATH:$HADOOP_INSTALL/bin 5. 设置hadoop配置文件hadoop官网参看 core-site.xml1234567891011121314151617&lt;?xml version="1.0"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;property&gt; &lt;!-- namenode RPC交互端口 --&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;final&gt;true&lt;/final&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 临时目录设定 --&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/hadoop/tmp&lt;/value&gt; &lt;description&gt;temp dir&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml12345678910111213141516171819202122232425&lt;?xml version="1.0"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;property&gt; &lt;!--存贮在本地的名字节点数据镜象的目录,作为名字节点的冗余备份--&gt; &lt;name&gt;dfs.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/hadoop/name&lt;/value&gt; &lt;final&gt;true&lt;/final&gt; &lt;/property&gt; &lt;property&gt; &lt;!--数据节点的块本地存放目录--&gt; &lt;name&gt;dfs.data.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/hadoop/data&lt;/value&gt; &lt;final&gt;true&lt;/final&gt; &lt;/property&gt; &lt;property&gt; &lt;!--缺省的块复制数量--&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;final&gt;true&lt;/final&gt; &lt;/property&gt;&lt;/configuration&gt; mared-site.xml12345678910&lt;?xml version="1.0"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt; &lt;property&gt; &lt;!--作业跟踪管理器是否和MR任务在一个进程中--&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;192.168.149.135:9001&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; masters 和 slaves 再在hadoop-env.sh设置jdk路径 6. 通过ssh将配置同步到各节点1234567# 将hadoop的文件复制到其他节点上scp -r ./hadoop slave1:~scp -r ./hadoop slave2:~#将配置文件也一起复制过去,也可以手动配置，防止你的环境配置有其他配置项scp -r ~/.bashrc slave1:~scp -r ~/.bashrc slave2:~ 7. 格式化namenode12# 这一步在主结点 master 上进行操作:hadoop namenode -format 8. 启动hadoop，jps和网站检查运行情况1234#hadoop1.2.1启动，因为实验环境可能低版本比较，最新版用其他方式启动。start-all.sh jps #查看进程状态#网站查看运行状态 http://master:50030 http://master:50070 master换成master机器的ip 其他1. HDFS常用操作hadoopdfs -ls 列出HDFS下的文件hadoop dfs -ls in 列出HDFS下某个文档中的文件hadoop dfs -put test1.txt test 上传文件到指定目录并且重新命名，只有所有的DataNode都接收完数据才算成功hadoop dfs -get in getin 从HDFS获取文件并且重新命名为getin，同put一样可操作文件也可操作目录hadoop dfs -rmr out 删除指定文件从HDFS上hadoop dfs -cat in/ 查看HDFS上in目录的内容hadoop dfsadmin -report 查看HDFS的基本统计信息，结果如下hadoop dfsadmin -safemode leave 退出安全模式hadoop dfsadmin -safemode enter *进入安全模式 2. 添加节点可扩展性是HDFS的一个重要特性，首先在新加的节点上安装hadoop，然后修改$HADOOP_HOME/conf/master文件，加入 NameNode主机名，然后在NameNode节点上修改$HADOOP_HOME/conf/slaves文件，加入新加节点主机名，再建立到新加节点无密码的SSH连接 3. shell自动安装脚本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122#!/bin/bash #validate user or group validate() &#123; if [ 'id -u' == 0 ];then echo "must not be root!" exit 0 else echo "---------welcome to hadoop---------" fi &#125; #hadoop install hd-dir() &#123; if [ ! -d /home/hadoop/ ];then mkdir /home/hadoop/ else echo "download hadoop will begin" fi &#125; download-hd() &#123; wget -c http://archive.apache.org/dist/hadoop/core/hadoop-1.2.1.tar.gz -O /home/hadoop/hadoop-1.2.1.tar.gz tar -xzvf /home/hadoop/hadoop-1.2.1.tar.gz -C /home/hadoop rm /home/hadoop/hadoop-1.2.1.tar.gz Ln -s /home/hadoop/hadoop-1.2.1 /home/hadoop/hadoop1.2.1 &#125; download-java() &#123; wget -c wget -c http://download.oracle.com/otn-pub/java/jdk/7/jdk-7-linux-i586.tar.gz -O /home/hadoop/jdk-7-linux-i586.tar.gz if [ ! -d /usr/lib/jvm ];then mkdir /usr/lib/jvm else tar -xzvf /home/hadoop/jdk-7-linux-i586.tar.gz -C /usr/lib/jvm rm /home/hadoop/jdk-7-linux-i586.tar.gz fi &#125; #hadoop conf hd-conf() &#123; echo "export JAVA_HOME=/usr/lib/jvm/java" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/hadoop-env.sh echo "#set path jdk" &gt;&gt; /home/hadoop/.bashrc echo "export JAVA_HOME=/usr/lib/jvm/java" &gt;&gt; /home/hadoop/.bashrc echo "#hadoop path" &gt;&gt; /home/hadoop/.bashrc echo "export HADOOP_HOME=/home/hadoop/hadoop1.2.1" &gt;&gt; /home/hadoop/.bashrc echo "PATH=$PATH:$HADOOP_HOME/bin:$JAVA_HOME/bin" &gt;&gt; /home/hadoop/.bashrc echo "HADOOP_HOME_WARN_SUPPRESS=1" &gt;&gt; /home/hadoop/.bashrc #hadoop core-site.xml echo "&lt;configuration&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/core-site.xml echo "&lt;property&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/core-site.xml echo "&lt;name&gt;fs.default.name&lt;/name&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/core-site.xml echo "&lt;value&gt;hdfs://master:9000" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/core-site.xml echo "&lt;/property&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/core-site.xml echo "&lt;property&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/core-site.xml echo "&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/core-site.xml echo "&lt;value&gt;/home/hadoop/tmp&lt;/value&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/core-site.xml echo "&lt;/property&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/core-site.xml echo "&lt;/configuration&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/core-site.xml #hadoop hdfs-site.xml echo "&lt;configuration&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/hdfs-site.xml echo "&lt;property&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/hdfs-site.xml echo "&lt;name&gt;dfs.name.dir&lt;/name&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/hdfs-site.xml echo "&lt;value&gt;/home/hadoop/name&lt;/value&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/hdfs-site.xml echo "&lt;/property&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/hdfs-site.xml echo "&lt;property&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/hdfs-site.xml echo "&lt;name&gt;dfs.data.dir&lt;/name&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/hdfs-site.xml echo "&lt;value&gt;/home/hadoop/data&lt;/value&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/hdfs-site.xml echo "&lt;/property&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/hdfs-site.xml echo "&lt;property&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/hdfs-site.xml echo "&lt;name&gt;dfs.replication&lt;/name&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/hdfs-site.xml echo "&lt;value&gt;1&lt;/value&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/hdfs-site.xml echo "&lt;/property&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/hdfs-site.xml echo "&lt;/configuration&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/hdfs-site.xml # hadoop mapred-site.xml echo "&lt;configuration&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/mapred-site.xml echo "&lt;property&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/mapred-site.xml echo "&lt;name&gt;mapred.job.tracker&lt;/name&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/mapred-site.xml echo "&lt;value&gt;master:9001&lt;/value&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/mapred-site.xml echo "&lt;/property&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/mapred-site.xml echo "&lt;/configuration&gt;" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/mapred-site.xml #hadoop master echo "hadoop-master" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/masters #hadoop slaves echo "hadoop-master" &gt;&gt; /home/hadoop/hadoop1.2.1/conf/slaves source /home/hadoop/.bashrc &#125; hd-start() &#123; hadoop namenode -format &#125; yes-or-no() &#123; echo "Is your name $* ?" while true do echo -n "Enter yes or no: " read x case "$x" in y | yes ) return 0;; n | no ) return 1;; * ) echo "Answer yes or no";; esac done &#125; echo "Original params are $*" if yes-or-no "$1" then echo "HI $1,nice name!" validate hd-dir download-hd download-java hd-conf else echo "Never mind!" fi]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[honeyc's Yst]]></title>
      <url>%2F2016%2F11%2F26%2Fhoney%2F</url>
      <content type="text"><![CDATA[我是凡人一个，冷漠或者炙热。如今，不用大喜，不必大悲，不求千秋，不问万岁。平平安安，顺畅平坦;万世千秋，唯求一人，执手笑看人间。所以，我若爱你，不是因为，而是因为你才爱。 2016-11-30]]></content>
    </entry>

    
  
  
</search>
